# -*- coding: utf-8 -*-
"""sentiment_keras_with_aspect.ipynb

Automatically generated by Colaboratory.


**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Flatten
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.layers import Dense ,LSTM,concatenate,Input,Flatten
import re
import random

"""Only keeping the necessary columns."""

data = pd.read_csv('data_2_train.csv', sep=',', header=0)
len_data = len(data)
print(data.size)
test_data = pd.read_csv('data_1_train.csv')
len_test_data = len(test_data)
# Keeping only the neccessary columns
print (data.columns.tolist())

data = data[['example_id', ' text',' class',' aspect_term']]
test_data = test_data[['example_id', ' text', ' aspect_term']]

data = data.append(test_data)
#print(len(df))
#df.to_csv('llll.csv', index=False)

data[' text'] = data[' text'].apply(lambda x: x.lower())
data[' text'] = data[' text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
#print(data[' aspect_term'].unique())
data[' aspect_term'] = data[' aspect_term'].apply(lambda x: x.lower())
data[' aspect_term'] = data[' aspect_term'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))



print("Class 1 size: ", end=' ')
print(data[ data[' class'] == 1].shape)
print("Class 0 size: ", end=' ')
print(data[ data[' class'] == 0].shape)
print("Class -1 size: ", end=' ')
print(data[ data[' class'] == -1].shape)


c = [-1, 0, 1]

max_fatures = 1000
tokenizer = Tokenizer(num_words=max_fatures, split=' ')
tokenizer.fit_on_texts(data[' text'].values)
X = tokenizer.texts_to_sequences(data[' text'].values)
X = pad_sequences(X)

print("X.shape: ", end=' ')
print(X.shape)

aspect_X = tokenizer.texts_to_sequences(data[' aspect_term'].values)

aspect_X = pad_sequences(aspect_X)

print("aspect_X.shape: ", end=' ')
print(aspect_X.shape)
print(aspect_X.shape[1])



embed_dim = 128
lstm_out = 196

# The LSTM network which takes in the proceessed input using embedding layer.
###shape_i = min(X.shape[1], test_X.shape[1])
input_1 = Input(shape=(X.shape[1], ))
word_embedding = Embedding(max_fatures, embed_dim,input_length = X.shape[1])(input_1)
drop_out_1 = SpatialDropout1D(0.4)(word_embedding)
lstm_1 = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)(drop_out_1)

output_1 = Dense(50,activation='relu')(lstm_1)

# The ASpect term is fed through a neural network

###aspect_shape_i = min(aspect_X.shape[1], test_aspect_X.shape[1])
input_2 = Input(shape=(aspect_X.shape[1], ))
word_embedding_2 = Embedding(max_fatures, embed_dim,input_length = X.shape[1])(input_2)
drop_out_2 = SpatialDropout1D(0.2)(word_embedding_2)
lstm_2 = LSTM(lstm_out, dropout=0.1, recurrent_dropout=0.1)(drop_out_2)

output_2 = Dense(20,activation='relu')(lstm_2)

merge = concatenate([output_1, output_2])
# interpretation model
hidden1 = Dense(25, activation='relu')(merge)
hidden2 = Dense(10, activation='relu')(hidden1)
output = Dense(3, activation='sigmoid')(hidden2)
model = Model(inputs=[input_1, input_2], outputs=output)


model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])



Y = pd.get_dummies(data[' class']).values


X_train, X_test,  aspect_X_train, aspect_X_test, Y_train, Y_test = train_test_split(X, aspect_X, Y, test_size = len_test_data, train_size = len_data, random_state = 42, shuffle = False)


batch_size = 16
model.fit([X_train, aspect_X_train], Y_train, epochs = 20, batch_size=batch_size, verbose = 2)
predictions = model.predict([X_test, aspect_X_test], verbose=1)
classes = np.argmax(predictions, axis=-1)
#classes = predictions.argmax(axis=-1)

with open('Output-Data-2-mod.txt', 'w') as o_file:	
	for i in range(len(test_data)):
		o_file.write(test_data.loc[i]['example_id'])
		o_file.write(';;')
		if i < len(classes):
			if classes[i] == 0:
				o_file.write('1')
			elif classes[i] == 1:
				o_file.write('0')
			elif classes[i] == 2:
				o_file.write('-1')
		else:
			o_file.write(str(random.choice(c)))
		o_file.write('\n')


#np.savetxt("foo.csv", predictions, delimiter=",")
print(predictions)
#print(stats.describe(classes))
print(classes)